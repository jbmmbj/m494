{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg1KhrrItLnkxpsROSKjr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbmmbj/m494/blob/main/MNISTExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install FrEIA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPJOO4buSxZh",
        "outputId": "db11d842-57f9-4963-a86c-3b56fb639c9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting FrEIA\n",
            "  Downloading FrEIA-0.2.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from FrEIA) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from FrEIA) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from FrEIA) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->FrEIA) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->FrEIA) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->FrEIA) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->FrEIA) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->FrEIA) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->FrEIA) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->FrEIA) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->FrEIA) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->FrEIA) (1.3.0)\n",
            "Building wheels for collected packages: FrEIA\n",
            "  Building wheel for FrEIA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FrEIA: filename=FrEIA-0.2-py3-none-any.whl size=42757 sha256=8ec65c986378bd097fed5b0b357c8dfaba3c0e76ae4851d23b4a9ad36ea5e1b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/a8/e2/d532a76f72108ac4a340cbe3f86b4f591abfdbd75209a5badb\n",
            "Successfully built FrEIA\n",
            "Installing collected packages: FrEIA\n",
            "Successfully installed FrEIA-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import FrEIA.framework as Ff\n",
        "import FrEIA.modules as Fm\n",
        "\n",
        "# Define a simple invertible neural network using FrEIA\n",
        "class InvertibleNN(nn.Module):\n",
        "    def subnet_fc(self, in_features, out_features):\n",
        "        hidden_size = self.hidden_size  # Access hidden_size from the parent class\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, out_features),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(InvertibleNN, self).__init__()\n",
        "\n",
        "        # Save hidden_size for later use in subnet_fc\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Build the architecture using FrEIA\n",
        "        self.inn = Ff.SequenceINN(input_size)\n",
        "        self.inn.append(Fm.AllInOneBlock, subnet_constructor=self.subnet_fc, permute_soft=True)\n",
        "        self.inn.append(Fm.PermuteRandom)\n",
        "\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the invertible neural network\n",
        "        x, _ = self.inn(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Function to perform inverse pass through the model\n",
        "def inverse_pass(model, output):\n",
        "    # Invert the entire model\n",
        "    inv_output, _ = model.inn(output, rev = True)\n",
        "\n",
        "    return inv_output\n",
        "\n",
        "# Load MNIST training data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_size = 28 * 28  # MNIST image size\n",
        "    hidden_size = 256\n",
        "    output_size = 10\n",
        "\n",
        "    model = InvertibleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            batch_data = batch_data.view(batch_data.size(0), -1)  # Flatten MNIST images\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            model.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            model.optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Load MNIST test data\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Test the model in the inverse direction\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            batch_data = batch_data.view(batch_data.size(0), -1)  # Flatten MNIST images\n",
        "            outputs = model(batch_data)\n",
        "            inv_outputs = inverse_pass(model, outputs)\n",
        "\n",
        "            print(\"Original Image:\")\n",
        "            print(\"Output from Forward Pass (Class Probabilities):\", outputs)\n",
        "            print(\"Reconstructed Image after Inverse Pass:\")\n",
        "            print(\"Inverse Output (Class Probabilities):\", inv_outputs)\n",
        "            break  # Break after processing the first batch for simplicity\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "zcz0nrfoa9L-",
        "outputId": "e695f1d8-6424-4e0f-b3a5-d278109f46d2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a85a3a6a1b91>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 784]], which is output 0 of TanhBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeH4zTcJgS4d",
        "outputId": "69a92248-cb1e-488f-a57e-932a3ed8033b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8510, -0.1294,  0.2157,\n",
            "          0.3647,  0.9922,  0.9922,  0.9922,  0.4745, -0.2078, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000,  0.2784,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n",
            "          0.9843,  0.9529, -0.4588, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,  0.9843,  0.9843,\n",
            "          0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843, -0.8431,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -0.3804,  0.9765,  0.9843,  0.8980,  0.2000,  0.0431,  0.0431,\n",
            "          0.3098,  0.9843,  0.9843, -0.1686, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4118, -0.3804,\n",
            "         -0.7412, -1.0000, -1.0000, -1.0000, -0.4510,  0.9843,  0.9843,  0.1608,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -0.4510,  0.9843,  0.9843,  0.7098, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -0.6941,  0.3333,  0.9843,  0.9843,  0.7098,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -0.8118, -0.0510,  0.5843,  0.9294,\n",
            "          0.9843,  0.9843,  0.9843, -0.0196, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,  0.1765,  0.5216,\n",
            "          0.7961,  0.9843,  0.9843,  0.9843,  0.9843,  0.9922,  0.9843,  0.6314,\n",
            "         -0.8118, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -0.3961,  0.4431,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n",
            "          0.9843,  0.9843,  0.9843,  0.9843,  0.4118, -0.8275, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000,  0.3412,  0.9843,  0.9843,  0.9843,\n",
            "          0.9843,  0.9843,  0.9843,  0.9686,  0.7333,  0.5529,  0.9608,  0.9843,\n",
            "          0.9843, -0.1451, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,\n",
            "          0.8431,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.6941, -0.1608,\n",
            "         -1.0000, -1.0000, -0.7647,  0.7098,  0.9843,  0.3412, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -0.6706,  0.5451,  0.9843,  0.9686,  0.8667,\n",
            "          0.0980, -0.5373, -0.8824, -1.0000, -1.0000, -1.0000, -1.0000,  0.0510,\n",
            "          0.9843,  0.7961, -0.8039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -0.5686,  0.4353, -0.3961, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000,  0.2706,  0.9843,  0.9843, -0.5765, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7804,\n",
            "         -0.3569, -0.7961, -0.9686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -0.9451, -0.4118,  0.5294,  0.9686,\n",
            "          0.9922,  0.8118, -0.7804, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -0.7020,  0.8824,  0.9843,  0.4824,  0.4039,\n",
            "         -0.2941, -0.5765, -0.8980, -0.6000, -0.6235, -0.3725, -0.3725, -0.0118,\n",
            "          0.5686,  0.9843,  0.9843,  0.9843,  0.9843,  0.3882, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          0.1216,  0.9843,  0.9922,  0.9843,  0.9843,  0.9373,  0.8588,  0.9294,\n",
            "          0.9294,  0.9843,  0.9922,  0.9843,  0.9843,  0.9922,  0.9843,  0.9922,\n",
            "          0.8902, -0.5686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -0.9765,  0.4667,  0.9843,  0.9843,\n",
            "          0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n",
            "          0.9843,  0.9922,  0.9843,  0.5059, -0.6706, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -0.9529,  0.4510,  0.9843,  0.9843,  0.9843,  0.9843,  0.9922,\n",
            "          0.9843,  0.9843,  0.9843,  0.9843,  0.8902,  0.2471, -0.1451, -0.9059,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9608, -0.5765,\n",
            "         -0.1686,  0.2000,  0.2000,  0.8039,  0.9843,  0.6627,  0.2000, -0.1451,\n",
            "         -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(batch_data)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULXgxJhmhP30",
        "outputId": "2010302a-8ca7-40d1-cfe9-aa35124cc76f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4.7770e-17, 4.4761e-18, 1.2245e-07, 1.0000e+00, 1.2594e-21, 1.0939e-13,\n",
              "         1.9797e-17, 1.0807e-23, 1.1499e-17, 3.4565e-15]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import FrEIA.framework as Ff\n",
        "import FrEIA.modules as Fm\n",
        "\n",
        "# Define a simple invertible neural network using FrEIA\n",
        "class InvertibleNN(nn.Module):\n",
        "    def subnet_fc(self, in_features, out_features):\n",
        "        hidden_size = self.hidden_size  # Access hidden_size from the parent class\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, out_features),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(InvertibleNN, self).__init__()\n",
        "\n",
        "        # Save hidden_size for later use in subnet_fc\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Build the architecture using FrEIA\n",
        "        self.inn = Ff.SequenceINN(input_size)\n",
        "\n",
        "        self.inn.append(Fm.GLOWCouplingBlock, subnet_constructor = self.subnet_fc)\n",
        "\n",
        "        self.inn.append(Fm.PermuteRandom)\n",
        "\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the invertible neural network\n",
        "        x, _ = self.inn(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def inverse_pass(model, output):\n",
        "    # Invert the entire model\n",
        "    inv_output, _ = model(output, rev=True)\n",
        "\n",
        "    return inv_output\n",
        "\n",
        "# Load MNIST training data\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_size = 28 * 28  # MNIST image size\n",
        "    hidden_size = 256\n",
        "    output_size = 10\n",
        "\n",
        "    model = InvertibleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_data, batch_labels in train_loader:\n",
        "            batch_data = batch_data.view(batch_data.size(0), -1)  # Flatten MNIST images\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "\n",
        "            model.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            model.optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Load MNIST test data\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "    # Test the model in the inverse direction\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in test_loader:\n",
        "            batch_data = batch_data.view(batch_data.size(0), -1)  # Flatten MNIST images\n",
        "            outputs = model(batch_data)\n",
        "            inv_outputs = inverse_pass(model, outputs)\n",
        "\n",
        "            print(\"Original Image:\")\n",
        "            print(\"Output from Forward Pass (Class Probabilities):\", outputs)\n",
        "            print(\"Reconstructed Image after Inverse Pass:\")\n",
        "            print(\"Inverse Output (Class Probabilities):\", inv_outputs)\n",
        "            break  # Break after processing the first batch for simplicity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "4kLDaYwihilR",
        "outputId": "4e060324-2cb6-435e-d770-ec6e659942e4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8dc7486fd233>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInvertibleNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-8dc7486fd233>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOWCouplingBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubnet_constructor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubnet_fc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvertibleModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPermuteRandom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/FrEIA/modules/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims_in, dims_c)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdims_c\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdims_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    }
  ]
}